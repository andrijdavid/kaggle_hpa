{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "\n",
    "import cv2\n",
    "\n",
    "from utils import *\n",
    "from dark import *\n",
    "from wrn4 import *\n",
    "\n",
    "# from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from imba_sampler import ImbalancedDatasetSampler\n",
    "\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKS = 'train.csv'\n",
    "\n",
    "PATH = Path('./')\n",
    "TRAIN = Path('train512/')\n",
    "TEST = Path('test512/')\n",
    "TRAIN_AUG = Path('../aug/train/')\n",
    "\n",
    "SAMPLE = Path('sample_submission.csv')\n",
    "\n",
    "seg = pd.read_csv(PATH/MASKS)\n",
    "seg_aug = pd.read_csv('../aug/augment.csv', header=None)\n",
    "seg_aug.columns = seg.columns\n",
    "sample_sub = pd.read_csv(PATH/SAMPLE)\n",
    "train_names = list(seg.Id.values)\n",
    "test_names = list(sample_sub.Id.values)\n",
    "\n",
    "classes = [str(l) for l in range(28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seg),len(seg_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(seg, seg_aug,'outer'); len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet444_3c(pretrained=True):\n",
    "    if pretrained:\n",
    "        m = Darknet([1,2,4,4,4], 28, 32, se=True, nf_in=4)\n",
    "        model = create_body(arch)\n",
    "        sd = torch.load('models/dk_444_se_256.pth', map_location=lambda storage, loc: storage)\n",
    "        names = set(model.state_dict().keys())\n",
    "        for n in list(sd.keys()): # list \"detatches\" the iterator\n",
    "                if n not in names and n+'_raw' in names:\n",
    "                    if n+'_raw' not in sd: sd[n+'_raw'] = sd[n]\n",
    "                    del sd[n]\n",
    "        model.load_state_dict(sd)\n",
    "        weight = model.layers[0][0].weight\n",
    "        model.layers[0][0] = nn.Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        model.layers[0][0].weight = nn.Parameter(weight[:, :3, :,:])\n",
    "    else:\n",
    "        model = Darknet([1,2,4,4,4], 28, 32, se=True, nf_in=3)\n",
    "        apply_init(model, nn.init.kaiming_normal_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ([0.08069, 0.05258, 0.05487], [0.13704,0.10145, 0.15313])\n",
    "tfms = get_transforms(do_flip=True, flip_vert=True, max_lighting=0.1, max_warp=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz=64, bs=64, pct=0.2, sample=5000):\n",
    "    src = (ImageItemList.from_df(df=df, path=PATH, folder=TRAIN_AUG, suffix='.png')\n",
    "           .random_split_by_pct(pct)\n",
    "           .label_from_df(sep=' ', classes=classes)\n",
    "           .add_test([TEST/f for f in test_names]))\n",
    "    data = (src.transform(tfms, size=sz))\n",
    "    test_ds = data.test\n",
    "    train_ds, val_ds = data.train, data.valid\n",
    "    datasets = [train_ds,val_ds, test_ds]\n",
    "    sampler = ImbalancedDatasetSampler(datasets[0], num_samples=sample)\n",
    "    train_dl = DataLoader(datasets[0], bs, sampler=sampler, num_workers=12)\n",
    "    val_dl = DataLoader(datasets[1], 2*bs, False, num_workers=8)\n",
    "    test_dl = DataLoader(datasets[2], 2*bs, False, num_workers=8)\n",
    "\n",
    "    return ImageDataBunch(train_dl, val_dl, test_dl).normalize(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(12,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(data, focal=False, fp16=False):\n",
    "    def _dark_split(m): return (m.layers[11],m.layers[20])\n",
    "    learn = create_cnn(data, darknet444_3c, pretrained=True, cut=-3, split_on=_dark_split, metrics=[accuracy_thresh, f1], callback_fns=[partial(GradientClipping, clip=0.1), BnFreeze, ShowGraph])\n",
    "    if focal: \n",
    "        learn.loss_func = FocalLoss()\n",
    "    if fp16: learn.to_fp16();\n",
    "    return learn.mixup(stack_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(256, 24, 0.01, sample=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(data, focal=True, fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model = nn.DataParallel(learn.model)\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(5, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(5, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('dk_444_se_256_3c');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(5 slice(lr/100, lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(5, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('dk_444_se_256_3c');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learner(data, focal=True, fp16=True)\n",
    "learn.load('dk_444_se_256_3c');\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(5, slice(lr/100, lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(5, slice(lr/100, lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(5, slice(lr/100, lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(5, slice(lr/100, lr/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('dk_444_se_256_3c');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.test_dl.add_tfm(to_half)\n",
    "p,t = learn.get_preds(ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute optimal THS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data preds\n",
    "learn.data.train_dl.add_tfm(to_half)\n",
    "p_v, t_v = learn.get_preds(ds_type=DatasetType.Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_v[0]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_v_s = p_v.sigmoid()\n",
    "p_v_np, t_v_np = to_np(p_v_s), to_np(t_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = find_thresh(p_v_np, t_v_np); ths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ths=array([0.52, 0.42, 0.34, 0.5 , 0.46, 0.38, 0.36, 0.41, 0.35, 0.4 , 0.41, 0.4 , 0.39, 0.43, 0.4 , 0.39, 0.37, 0.39,\n",
    "       0.4 , 0.41, 0.46, 0.36, 0.31, 0.41, 0.49, 0.39, 0.31, 0.5 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_np(p_v_np, t_v_np, ths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = to_np(p.sigmoid())  #Check if we are using focal loss or BCE.\n",
    "threshold = 0.4 #ths\n",
    "print(preds.shape)\n",
    "# classes = np.array(data.classes)\n",
    "classes = array(['16', '0', '7', '1', '2', '5', '18', '25', '23', '21', '24', '6', '11', '3', '12', '13', '14', '4', '20', '22',\n",
    "       '17', '19', '8', '9', '10', '26', '27', '15'], dtype='<U2')\n",
    "res = np.array([\" \".join(classes[(np.where(pp>threshold))])for pp in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.array([test_names, res]).T, columns = ['Id','Predicted'])\n",
    "frame.to_csv('protein_se.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
